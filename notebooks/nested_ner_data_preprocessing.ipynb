{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = '../data/ace2005/ace2005.train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = '../data/glove/glove.6B.100d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_read(fpath):\n",
    "    with open(fpath) as f:\n",
    "        data = f.readlines()\n",
    "\n",
    "    output = []\n",
    "    for raw_snt, labels in zip(data[::3], data[1::3]):\n",
    "        raw_snt, labels = raw_snt.strip(), labels.strip()\n",
    "\n",
    "        words = raw_snt.split()\n",
    "        start_position = np.cumsum([0] + [len(w) + 1 for w in words])\n",
    "        subtoks = tokenizer.tokenize(raw_snt)\n",
    "        assert sum(len(s) for s in subtoks) == len(raw_snt)\n",
    "\n",
    "        # label to character-indexed.\n",
    "        if labels == \"\":\n",
    "            labels, entities = [], []\n",
    "        else:\n",
    "            labels = labels.split(\"|\")\n",
    "            _labels, entities = [], []\n",
    "            for label in labels:\n",
    "                position, tag = label.split()\n",
    "                l, r = list(map(int, position.split(\",\")))\n",
    "                _labels.append((start_position[l], start_position[r] - 1, tag))\n",
    "                entities.append(\" \".join(words[l:r]))\n",
    "            labels = _labels\n",
    "\n",
    "        # align label to subwords\n",
    "        char_left, char_right = [], []\n",
    "        for i, tok in enumerate(subtoks):\n",
    "            char_left.extend([i] * len(tok))\n",
    "            char_right.extend([i + 1] * len(tok))\n",
    "        _labels = []\n",
    "        for label, entity_surface in zip(labels, entities):\n",
    "            _labels.append(\n",
    "                (char_left[label[0]], char_right[label[1] - 1], label[2])\n",
    "            )\n",
    "\n",
    "            # sanity check\n",
    "            recovery = \"\".join(\n",
    "                subtoks[char_left[label[0]] : char_right[label[1] - 1]]\n",
    "            )\n",
    "            recovery = recovery.replace(\"Ä \", \" \").lstrip()\n",
    "            assert recovery == entity_surface\n",
    "        labels = _labels\n",
    "\n",
    "        instance = {\n",
    "            \"id\": len(output),\n",
    "            \"snt\": subtoks,\n",
    "            \"labels\": labels,\n",
    "        }\n",
    "\n",
    "        output.append(instance)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = simple_read(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len, max_snt_len, labelset = 0, 0, Counter()\n",
    "for inst in data:\n",
    "    max_snt_len = max(max_snt_len, len(inst[\"snt\"]))\n",
    "    for label in inst[\"labels\"]:\n",
    "        length = label[1] - label[0]\n",
    "        max_len = max(max_len, length)\n",
    "    labelset.update(label[2] for label in inst[\"labels\"])\n",
    "print(\"max_span_width:\", max_len)\n",
    "print(\"max_snt_length:\", max_snt_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labelset.most_common())\n",
    "with open(\"../data/resources/ace2005.label_vocab.txt\", \"w\") as f:\n",
    "    for w, c in labelset.most_common():\n",
    "        f.write(f\"{w} {c}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Static word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(embedding) as f:\n",
    "    embedding = f.readlines()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "beed5a18cd649d2060c0578a37179f4501615bd19b07ef40a455d3c62da0e763"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('amr')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

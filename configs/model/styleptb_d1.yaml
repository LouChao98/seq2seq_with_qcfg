_target_: src.models.general_seq2seq.GeneralSeq2SeqModule
_recursive_: false

embedding:
  _target_: torch.nn.Embedding
  embedding_dim: 300

encoder:
  _target_: src.models.components.encoder.BiAugmentedLstm
  hidden_size: 256
  bidirectional: true
  recurrent_dropout_probability: 0.2
  num_layers: 1

parser:
  _target_: src.models.src_parser.neural_pcfg.NeuralPCFGSrcParser
  dim: 256  # this is independent to other modules
  pt_states: 20
  nt_states: 20
  num_layers: 2

tree_encoder:
  _target_: src.models.tree_encoder.treelstm.BinaryTreeLSTM

decoder:
  _target_: src.models.tgt_parser.neural_qcfg_d1.NeuralQCFGD1TgtParser
  num_layers: 3
  dim: 512
  nt_states: 8 # src_nt_states in origin impl
  pt_states: 8 # src_pt_states in origin impl
  rule_constraint_type: 1
  use_copy: true
  num_samples: 8
  check_ppl: true
  cpd_rank: 64
  nt_span_range: [1, 1000]
  pt_span_range: [1, 1000]

test_metric:
  _target_: src.utils.metric.MultiMetric
  bleu-1:
    _target_: torchmetrics.BLEUScore
    n_gram: 1
    compute_on_step: false
  bleu-2:
    _target_: torchmetrics.BLEUScore
    n_gram: 2
    compute_on_step: false
  bleu-3:
    _target_: torchmetrics.BLEUScore
    n_gram: 3
    compute_on_step: false
  bleu-4:
    _target_: torchmetrics.BLEUScore
    n_gram: 4 
    compute_on_step: false


param_initializer: "xavier_uniform"
track_param_norm: false

optimizer:
  groups: ~
  args:
    _target_: torch.optim.Adam
    lr: 5.0e-4
    betas: [0.75, 0.999]
    weight_decay: 1.0e-5
    eps: 1.0e-8

scheduler: ~

_target_: src.models.pretrain_seq2seq.PretrainSeq2SeqModule
_recursive_: false

param_initializer: "xavier_uniform"

lm:
  _target_: src.models.components.rnn_lm.RNNModel
  tok_emb: 256
  tok_hid: 256
  nlayers: 1

hidden_size: 256
num_layers: 2
dropout: 0.5

optimizer:
  groups: ~
  args:
    _target_: torch.optim.AdamW
    lr: 5.0e-4
    betas: [0.75, 0.999]
    weight_decay: 1.0e-5
    eps: 1.0e-8

scheduler:
  interval: step
  frequency: 1
  monitor: val/ppl
  args:
    _target_: src.utils.scheduler.get_exponential_lr_scheduler
    gamma: 0.75**(1/5000)

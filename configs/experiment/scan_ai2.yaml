# @package _global_

# to execute this experiment run:
# python train.py experiment=example

defaults:
  - override /datamodule: scan.yaml
  - override /model: scan.yaml
  - override /callbacks: default.yaml
  - override /logger: wandb
  - override /trainer: default.yaml

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

# name of the run determines folder name in logs
name: "scan"


# ~17GB
datamodule:
  # batch_size: 1
  # eval_batch_size: 1
  batch_size: 1
  eval_batch_size: 5  # 12GB is safe
  max_src_len: 100

model:
  track_param_norm: true
  decoder:
    num_samples: 10
  
  optimizer:
    args:
      _target_: torch.optim.AdamW

trainer:
  max_epochs: 10
  accumulate_grad_batches: 4
  gradient_clip_val: 3
  track_grad_norm: 2

logger:
  wandb:
    project: qcfg_scan
    offline: true
    tags: ["weight_decay"]

# train: false